import streamlit as st
import re
from utils import process_text_with_tooltips

def render_chat_history(history, source_mode="chat"):
    """
    Renderiza el historial con el est√°ndar de invisibilidad sistem√°tica. 
    Mantiene la integridad de los datos para el bot√≥n de referencias.
    """
    if not history:
        return

    for msg in history:
        role = msg["role"]
        content = msg["content"] 
        avatar = "‚ú®" if role == "assistant" else "üë§"
        
        with st.chat_message(role, avatar=avatar):
            if role == "assistant":
                # --- FILTRO DE INVISIBILIDAD SISTEM√ÅTICA ---
                # 1. Separamos el contenido anal√≠tico de los metadatos t√©cnicos
                display_text = re.split(r'\|\|\|', content)[0]
                
                # 2. Eliminamos rastro de archivos con prefijos t√©cnicos (In-ATL_)
                display_text = re.split(r'\d{2,4}-\d{1,2}-\d{1,2}_In-ATL_.*?\.pdf', display_text, flags=re.IGNORECASE)[0]
                
                # 3. Eliminamos secciones de fuentes duplicadas que ensucian el chat
                display_text = re.split(r'\n\s*(\*\*|##)?\s*(Fuentes|Referencias|Bibliograf√≠a)', display_text, flags=re.IGNORECASE)[0]

                # 4. Limpieza de seguridad para asegurar que no queden corchetes t√©cnicos vac√≠os
                display_text = display_text.strip()
                
                # Renderizado con Tooltips inteligentes
                html_content = process_text_with_tooltips(display_text)
                st.markdown(html_content, unsafe_allow_html=True)
            else:
                st.markdown(content)

def handle_chat_interaction(prompt, response_generator_func, history_key, source_mode, on_generation_success=None):
    """
    Orquestador de interacci√≥n: garantiza que la respuesta robusta 
    se capture √≠ntegramente antes del renderizado visual.
    """
    # Guardamos la consulta del usuario en el estado del modo actual
    st.session_state.mode_state[history_key].append({"role": "user", "content": prompt})
    
    with st.chat_message("user", avatar="üë§"):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar="‚ú®"):
        full_response = ""
        # Placeholder para el efecto de escritura (streaming)
        placeholder = st.empty()
        
        # Llamada al motor de IA optimizado para 8,192 tokens
        stream = response_generator_func()
        
        if stream:
            for chunk in stream:
                full_response += chunk
                # Visualizaci√≥n progresiva (Solo mostramos el texto limpio durante el stream)
                clean_chunk_display = re.split(r'\|\|\|', full_response)[0]
                placeholder.markdown(clean_chunk_display + "‚ñå")
            
            # GUARDADO MAESTRO: Preservamos la cadena completa con separadores t√©cnicos
            # Esto es lo que permite que el modal extraiga las fuentes √∫nicas.
            st.session_state.mode_state[history_key].append({
                "role": "assistant", 
                "content": full_response
            })
            
            if on_generation_success:
                on_generation_success(full_response)
            
            # Forzamos refresco para estabilizar la UI y mostrar la barra de acciones final
            st.rerun() 
            return full_response
