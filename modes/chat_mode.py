import streamlit as st
import time
import constants as c

# --- IMPORTACIONES SEGURAS ---
try:
    from services.gemini_api import call_gemini_stream
    from utils import get_relevant_info, render_process_status, process_text_with_tooltips
    from prompts import get_grounded_chat_prompt
    from services.supabase_db import log_query_event
    from services.memory_service import save_project_insight 
    from config import banner_file
except ImportError as e:
    st.error(f"Error importando m√≥dulos del chat: {e}")
    st.stop()

# Importaci√≥n condicional para PDF (para que no rompa si falla reportlab)
try:
    from reporting.pdf_generator import generate_pdf_html
except ImportError:
    generate_pdf_html = None # Deshabilitamos PDF si falla la librer√≠a

def grounded_chat_mode(db, selected_files):
    st.subheader("Chat de Consulta Directa")
    st.caption("Respuestas precisas basadas estrictamente en tu documentaci√≥n.")

    if not selected_files:
        st.info("üëà Selecciona documentos en el men√∫ lateral para comenzar.")
        return

    # 1. INICIALIZAR HISTORIAL
    if "chat_history" not in st.session_state.mode_state:
        st.session_state.mode_state["chat_history"] = []

    # 2. MOSTRAR HISTORIAL
    for idx, msg in enumerate(st.session_state.mode_state["chat_history"]):
        role_avatar = "‚ú®" if msg["role"] == "assistant" else "üë§"
        with st.chat_message(msg["role"], avatar=role_avatar):
            # A. Mostrar contenido procesado (Tooltips)
            if msg["role"] == "assistant":
                content_html = process_text_with_tooltips(msg["content"])
                st.markdown(content_html, unsafe_allow_html=True)
                
                # B. BOT√ìN PIN MINIMALISTA (Protegido)
                col_spacer, col_pin = st.columns([15, 1])
                with col_pin:
                    if st.button("üìå", key=f"pin_hist_{idx}", help="Guardar en Bit√°cora"):
                        try:
                            if save_project_insight(msg["content"], source_mode="chat"):
                                st.toast("‚úÖ Guardado")
                                time.sleep(1) 
                                st.rerun()    
                        except Exception as e:
                            st.error(f"No se pudo guardar: {e}")
            else:
                st.markdown(msg["content"])

    # 3. INPUT DEL USUARIO
    if user_input := st.chat_input("Haz una pregunta sobre tus documentos..."):
        
        # A. Guardar pregunta
        st.session_state.mode_state["chat_history"].append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üë§"):
            st.markdown(user_input)

        # B. Generar Respuesta
        with st.chat_message("assistant", avatar="‚ú®"):
            full_response = ""
            placeholder = st.empty()
            
            # Usamos un contenedor seguro para el proceso de IA
            try:
                with render_process_status("Consultando base de conocimientos...", expanded=True) as status:
                    relevant_info = get_relevant_info(db, user_input, selected_files)
                    
                    if not relevant_info:
                        status.update(label="Sin informaci√≥n relevante en los documentos seleccionados.", state="error")
                        full_response = "No encontr√© informaci√≥n relevante en los documentos seleccionados para responder tu pregunta."
                        placeholder.markdown(full_response)
                    else:
                        hist_str = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.mode_state["chat_history"][-5:]])
                        prompt = get_grounded_chat_prompt(hist_str, relevant_info)
                        
                        stream = call_gemini_stream(prompt)
                        
                        if stream:
                            status.update(label="Generando respuesta...", state="running")
                            for chunk in stream:
                                full_response += chunk
                                placeholder.markdown(full_response + "‚ñå")
                            status.update(label="Listo", state="complete", expanded=False)
                        else:
                            status.update(label="Error de conexi√≥n con IA", state="error")
                            full_response = "Lo siento, hubo un problema de conexi√≥n con el servicio de IA."
            
            except Exception as e:
                full_response = f"Ocurri√≥ un error inesperado: {str(e)}"
                placeholder.error(full_response)
            
            # C. Render Final + PIN NUEVO
            placeholder.empty()
            final_html = process_text_with_tooltips(full_response)
            st.markdown(final_html, unsafe_allow_html=True)
            
            # Bot√≥n Pin Minimalista para la respuesta nueva
            col_spacer_new, col_pin_new = st.columns([15, 1])
            with col_pin_new:
                if st.button("üìå", key="pin_new_resp", help="Guardar en Bit√°cora"):
                    try:
                        if save_project_insight(full_response, source_mode="chat"):
                            st.toast("‚úÖ Guardado")
                            time.sleep(1)
                            st.rerun()
                    except: pass

            st.session_state.mode_state["chat_history"].append({"role": "assistant", "content": full_response})
            try: log_query_event(user_input, mode=c.MODE_CHAT)
            except: pass

    # 4. BOTONES EXPORTAR
    if st.session_state.mode_state["chat_history"]:
        st.write("")
        col1, col2 = st.columns(2)
        
        # Generaci√≥n de PDF Protegida
        pdf_bytes = None
        if generate_pdf_html: # Solo si la librer√≠a carg√≥ bien
            try:
                chat_text = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.mode_state["chat_history"]])
                pdf_bytes = generate_pdf_html(chat_text, title="Historial Chat", banner_path=banner_file)
            except Exception as e:
                # Si falla el PDF, solo mostramos aviso en consola, no rompemos la UI
                print(f"Error generando PDF: {e}")

        with col1:
            if pdf_bytes:
                st.download_button("Descargar PDF", data=pdf_bytes, file_name="chat_historial.pdf", mime="application/pdf", use_container_width=True)
            elif generate_pdf_html is None:
                st.warning("Exportar PDF no disponible (faltan librer√≠as)")
        
        with col2:
            if st.button("Limpiar Chat", use_container_width=True):
                st.session_state.mode_state["chat_history"] = []
                st.rerun()
